### Robots.txt Specifications

- World Wide Web Consortium recommendation http://www.w3.org/TR/html4/appendix/notes.html#h-B.4.1.1
- by Google https://developers.google.com/webmasters/control-crawl-index/docs/robots_txt
- by MOZ http://moz.com/learn/seo/robotstxt

### Parts

- 'robotstxt' hook/other plugins' output [x] $public = get_option( 'blog_public' );
                                             apply_filters( 'robots_txt', $output, $public );
- manual records [x]
- remote records [x] a URL, defaults to: plugin-URL/badbots.txt

### Notes

- vs_api: needs to save once to create the option -> admin_notice
- vs_api: option autoload on/off option
- vs_api: required fields
- vs_api: settings: return admin_notice
- top part: # This robots.txt is generated by Multipart robots.txt editor http://url
- Sitemap examples: $home/sitemap.xml $home/sitemap_index.xml
- recommended sitemaps: http://smythies.com/robots.txt http://www.lemgo.net/robots.txt
- separator \n ################################## \n
- add_filters( 'robots_txt', '$output, $public' 2, 99999 );
- core adds
    $site_url = parse_url( site_url() );
    $path = ( !empty( $site_url['path'] ) ) ? $site_url['path'] : '';
    $output .= "Disallow: $path/wp-admin/\n";
- don't run on '0' == $public
- admin notice in case of subdir, parse_url(home URL)
- one non-autoload option array
- 1 day transient
- preview link/button
- file creation instruction: wget -O ABSPATH . "robots.txt" home . "robots.txt"
- make up a list with urls of useful bots: google,bing,baidu,yandex
- categories: search, advertisment, social, monitoring, SEO, other, archiver, tool, feed
Googlebot\|Googlebot-Image\|Feedfetcher-Google\|AdsBot-Google\|Googlebot-Mobile\|
bingbot\|BingPreview\|zerigo\.com/watchdog\|facebookexternalhit\|YandexBot\|
MJ12bot\|AhrefsBot\|YandexImages\|yandex.com/bots\|
ia_archiver\|Wget\|Feedstripes\|msnbot\|Baiduspider\|Yahoo\! Slurp
- delete records: register_uninstall_hook($file, $callback)

1. only one "User-agent: *" -> join those: "Blank lines are not permitted within a single record in the "robots.txt" file."
1. If the value is "*" ... It is not allowed to have multiple such records in the "/robots.txt" file.
2. At least one "Disallow" field must be present in the robots.txt file.
2. check for that
